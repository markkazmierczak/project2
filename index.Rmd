---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

Mark Kazmierczak, mjk2299

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
affairs <- read_csv("~/project2/Affairs (1).csv")

# if your dataset needs tidying, do so here
infidelity <- affairs %>% select(-1) %>% mutate(faithful = ifelse(affairs > 0 , 0, 1))




# any other code here
```

### Cluster Analysis

```{R}
library(cluster)
library(GGally)

# Determine k
sil_width <- vector()
for (i in 2:10) {
    pam_fit <- pam(infidelity, diss = TRUE, k = i)
    sil_width[i] <- pam_fit$silinfo$avg.width
}

ggplot() + geom_line(aes(x = 1:10, y = sil_width)) + scale_x_continuous(name = "k", 
    breaks = 1:10)

#PAM
infidelity_pam <- pam(infidelity, k = 2)

infidelity_pam

infidelity_pam$silinfo$avg.width

#Pairwise Combinations
infidelity %>% mutate(cluster = as.factor(infidelity_pam$clustering)) %>% 
    head() %>% ggpairs(aes(color = cluster))

```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA

infidelity.pcadata <- infidelity %>% select(-10) %>% select_if( is.numeric)

infidelity.pca <- princomp(infidelity.pcadata, cor = T)

summary(infidelity.pca , loadings = T)

# visualize

library(ggplot2)

infidelity.pca.df <- infidelity.pca$scores %>% as.data.frame()

infidelity.pca.df <- infidelity.pca.df %>% mutate(affairs = as.factor(infidelity.pcadata$affairs))

infidelity.pca.df %>% ggplot(aes(Comp.1, Comp.2, color = affairs)) + 
  geom_point(size = 1) + xlab("PC1") + ylab("PC2") +
  scale_color_manual(values=c("black", "#F8766D", "#00BA38","#00A5FF", "#E76BF3", "Red" )) +
  theme_classic() + theme(plot.title = element_text(hjust = 0.5, face = "bold")) +
  ggtitle("PCA 1 and PCA 2") 
```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here

infidelity.glm <- glm(data= infidelity, faithful ~ age + rating+ yearsmarried + religiousness + education , family = "binomial")


faithful_prob <- predict(infidelity.glm, type = "response")

#class diag
class_diag(faithful_prob, truth = infidelity$faithful, positive = 1)

#confusion matrix

table(True = infidelity$faithful, Predicted = faithful_prob > 0.5  )

```

```{R}
# cross-validation of linear classifier here

k = 10
data <- sample_frac(infidelity)  
folds <- rep(1:k, length.out = nrow(data)) 
diags <- NULL

i = 1
for (i in 1:k) {
  # create training and test sets
  train <- data[folds != i, ]
  test <- data[folds == i, ]
  truth <- test$faithful
  

  fit <- infidelity.glm
  
  probs <- predict(fit, newdata = test, type = "response")
  
  diags <- rbind(diags, class_diag(probs, truth, positive = 1))
}


summarize_all(diags, mean)


```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here

library(rpart); library(rpart.plot)

fit<- rpart(data= infidelity, faithful ~ age + rating+ yearsmarried + religiousness + education)


fit <- train(data= infidelity, faithful ~ age + rating+ yearsmarried + religiousness + education, method="rpart")


fit$bestTune
rpart.plot(fit$finalModel)
```

```{R}
# cross-validation of np classifier here
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
```

```{R}
# cross-validation of regression model here
```

Discussion

### Python 

```{R}
library(reticulate) 

```

```{python}

#Defining the infidelity data frame in R


infidelity = r.infidelity



```

```{r}
# Displaying the python infidelity object in R as a data frame
py$infidelity %>% glimpse()

py$infidelity %>% as.data.frame() %>% select(1:6) %>% head(5)
```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




